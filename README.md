# ğŸŒ Projeto Final â€“ Pipeline de Dados com Airflow, Hadoop e Machine Learning

Este projeto implementa um pipeline completo de dados sÃ­smicos da API USGS, com orquestraÃ§Ã£o no Apache Airflow, armazenamento no HDFS (Hadoop), anÃ¡lise exploratÃ³ria e modelagem preditiva com scikit-learn.

## ğŸ” Pipeline

1. **ETL com Python**: coleta de dados da API USGS (Ãºltimos 5 anos)
2. **OrquestraÃ§Ã£o**: DAG diÃ¡ria com Airflow
3. **Armazenamento**: Hadoop HDFS
4. **AnÃ¡lise**: Jupyter Notebook com visualizaÃ§Ãµes e insights
5. **ML**: modelo Random Forest para prever severidade de terremotos

## ğŸ“‚ Estrutura

- `dags/` â€“ DAG Airflow
- `scripts/` â€“ Scripts de ETL, integraÃ§Ã£o com Hadoop e modelo salvo
- `notebooks/` â€“ AnÃ¡lise exploratÃ³ria e modelagem
- `dados/` â€“ Arquivos .csv (ou deixe vazio e cite o HDFS)

## ğŸ§  Modelo

- **Tipo**: ClassificaÃ§Ã£o binÃ¡ria (severo ou nÃ£o)
- **Algoritmo**: RandomForestClassifier
- **Atributos**: profundidade, latitude, longitude
- **MÃ©trica**: F1-score, matriz de confusÃ£o, precisÃ£o

## âš™ï¸ Requisitos

- Python 3.10+
- Apache Airflow
- Hadoop HDFS
- scikit-learn, pandas, seaborn, matplotlib
